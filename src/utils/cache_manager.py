"""
íˆ¬ëª…í•œ ìºì‹± ì‹œìŠ¤í…œ - ê¸°ì¡´ ì½”ë“œ ë™ì‘ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ì„±ëŠ¥ í–¥ìƒ
"""

import os
import pickle
import hashlib
import time
import logging
import sys
from pathlib import Path
from typing import Any, Optional, Callable, Dict

logger = logging.getLogger(__name__)

def get_executable_dir():
    """ì‹¤í–‰ íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ë°˜í™˜ (PyInstaller í˜¸í™˜)"""
    if getattr(sys, 'frozen', False):
        # PyInstallerë¡œ ë¹Œë“œëœ ì‹¤í–‰íŒŒì¼ì¸ ê²½ìš°
        return Path(sys.executable).parent
    else:
        # ê°œë°œ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
        return Path(__file__).parent.parent.parent

class TransparentCache:
    """ê¸°ì¡´ ì½”ë“œì— ì§€ì¥ ì—†ì´ íˆ¬ëª…í•˜ê²Œ ì‘ë™í•˜ëŠ” ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: str = "cache"):
        # ì‹¤í–‰íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        executable_dir = get_executable_dir()
        self.cache_dir = executable_dir / cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ìºì‹œ íƒ€ì…ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        (self.cache_dir / "skeleton").mkdir(exist_ok=True)
        (self.cache_dir / "ai_prediction").mkdir(exist_ok=True)
        (self.cache_dir / "clipping").mkdir(exist_ok=True)
        (self.cache_dir / "processing").mkdir(exist_ok=True)
        # ğŸ†• í´ë¦¬ê³¤ë³„ ì‚¬ìš©ì í¸ì§‘ ìƒíƒœ ìºì‹œ
        (self.cache_dir / "polygon_edit").mkdir(exist_ok=True)
        
        logger.info(f"ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {self.cache_dir}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ ê²½ë¡œì™€ ìˆ˜ì • ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì‹œ ìƒì„±"""
        try:
            stat = os.stat(file_path)
            # íŒŒì¼ ê²½ë¡œ + ìˆ˜ì • ì‹œê°„ + í¬ê¸°ë¡œ ê³ ìœ  ì‹ë³„
            content = f"{file_path}_{stat.st_mtime}_{stat.st_size}"
            return hashlib.md5(content.encode()).hexdigest()
        except:
            return hashlib.md5(file_path.encode()).hexdigest()
    
    def _get_cache_key(self, file_path: str, operation: str, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        file_hash = self._get_file_hash(file_path)
        # ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤ë„ í‚¤ì— í¬í•¨
        params_str = "_".join(f"{k}={v}" for k, v in sorted(kwargs.items()))
        return f"{operation}_{file_hash}_{hashlib.md5(params_str.encode()).hexdigest()[:8]}"
    
    def _get_cache_path(self, cache_type: str, cache_key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return self.cache_dir / cache_type / f"{cache_key}.pkl"
    
    def get(self, file_path: str, operation: str, **kwargs) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            cache_key = self._get_cache_key(file_path, operation, **kwargs)
            cache_path = self._get_cache_path(operation, cache_key)
            
            if cache_path.exists():
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                    logger.info(f"âœ… ìºì‹œ íˆíŠ¸: {operation} - {Path(file_path).name}")
                    return cached_data
            
        except Exception as e:
            logger.warning(f"ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        return None
    
    def set(self, file_path: str, operation: str, data: Any, **kwargs) -> None:
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        try:
            cache_key = self._get_cache_key(file_path, operation, **kwargs)
            cache_path = self._get_cache_path(operation, cache_key)
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
                logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {operation} - {Path(file_path).name}")
                
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def clear_old_cache(self, max_age_days: int = 30) -> None:
        """ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        try:
            current_time = time.time()
            max_age_seconds = max_age_days * 24 * 3600
            
            for cache_file in self.cache_dir.rglob("*.pkl"):
                if (current_time - cache_file.stat().st_mtime) > max_age_seconds:
                    cache_file.unlink(missing_ok=True)
                    
        except Exception as e:
            logger.warning(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    # ğŸ†• í´ë¦¬ê³¤ë³„ ì‚¬ìš©ì í¸ì§‘ ìƒíƒœ ìºì‹± ë©”ì„œë“œë“¤
    def get_polygon_cache_key(self, file_path: str, polygon_index: int = 0) -> str:
        """í´ë¦¬ê³¤ë³„ ê³ ìœ  ìºì‹œ í‚¤ ìƒì„±"""
        file_name = Path(file_path).stem
        file_hash = self._get_file_hash(file_path)[:12]  # ì§§ì€ í•´ì‹œ
        return f"{file_name}_p{polygon_index}_{file_hash}"
    
    def save_polygon_edit_state(self, file_path: str, polygon_index: int, edit_state: Dict) -> None:
        """í´ë¦¬ê³¤ë³„ ì‚¬ìš©ì í¸ì§‘ ìƒíƒœ ì €ì¥"""
        try:
            cache_key = self.get_polygon_cache_key(file_path, polygon_index)
            cache_path = self.cache_dir / "polygon_edit" / f"{cache_key}.pkl"
            
            # í¸ì§‘ ìƒíƒœì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            full_state = {
                'file_path': file_path,
                'polygon_index': polygon_index,
                'timestamp': time.time(),
                'edit_data': edit_state
            }
            
            with open(cache_path, 'wb') as f:
                pickle.dump(full_state, f)
                logger.info(f"ğŸ’¾ í´ë¦¬ê³¤ í¸ì§‘ ìƒíƒœ ì €ì¥: {Path(file_path).name} (í´ë¦¬ê³¤ {polygon_index + 1})")
                
        except Exception as e:
            logger.error(f"í´ë¦¬ê³¤ í¸ì§‘ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_polygon_edit_state(self, file_path: str, polygon_index: int = 0) -> Optional[Dict]:
        """í´ë¦¬ê³¤ë³„ ì‚¬ìš©ì í¸ì§‘ ìƒíƒœ ë¡œë“œ"""
        try:
            cache_key = self.get_polygon_cache_key(file_path, polygon_index)
            cache_path = self.cache_dir / "polygon_edit" / f"{cache_key}.pkl"
            
            if cache_path.exists():
                with open(cache_path, 'rb') as f:
                    full_state = pickle.load(f)
                    
                    # íŒŒì¼ ë³€ê²½ í™•ì¸ (íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì²´í¬)
                    cached_file_hash = self._get_file_hash(file_path)
                    cache_file_hash = self.get_polygon_cache_key(file_path, polygon_index).split('_')[-1]
                    
                    if cached_file_hash[:12] == cache_file_hash:
                        logger.info(f"âœ… í´ë¦¬ê³¤ í¸ì§‘ ìƒíƒœ ë¡œë“œ: {Path(file_path).name} (í´ë¦¬ê³¤ {polygon_index + 1})")
                        return full_state['edit_data']
                    else:
                        logger.info(f"ğŸ”„ íŒŒì¼ ë³€ê²½ë¨ - ìºì‹œ ë¬´íš¨í™”: {Path(file_path).name}")
                        cache_path.unlink(missing_ok=True)
            
        except Exception as e:
            logger.warning(f"í´ë¦¬ê³¤ í¸ì§‘ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def clear_polygon_cache(self, file_path: str = None) -> None:
        """í´ë¦¬ê³¤ ìºì‹œ ì‚­ì œ (íŠ¹ì • íŒŒì¼ ë˜ëŠ” ì „ì²´)"""
        try:
            polygon_cache_dir = self.cache_dir / "polygon_edit"
            
            if file_path:
                # íŠ¹ì • íŒŒì¼ì˜ í´ë¦¬ê³¤ ìºì‹œë§Œ ì‚­ì œ
                file_name = Path(file_path).stem
                for cache_file in polygon_cache_dir.glob(f"{file_name}_*.pkl"):
                    cache_file.unlink(missing_ok=True)
                logger.info(f"ğŸ—‘ï¸ í´ë¦¬ê³¤ ìºì‹œ ì‚­ì œ: {Path(file_path).name}")
            else:
                # ëª¨ë“  í´ë¦¬ê³¤ ìºì‹œ ì‚­ì œ
                for cache_file in polygon_cache_dir.glob("*.pkl"):
                    cache_file.unlink(missing_ok=True)
                logger.info("ğŸ—‘ï¸ ëª¨ë“  í´ë¦¬ê³¤ ìºì‹œ ì‚­ì œ")
                
        except Exception as e:
            logger.error(f"í´ë¦¬ê³¤ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
_global_cache = None

def get_cache() -> TransparentCache:
    """ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _global_cache
    if _global_cache is None:
        _global_cache = TransparentCache()
    return _global_cache

def cached_operation(operation_name: str, file_path: str = "", **cache_params):
    """ë°ì½”ë ˆì´í„°: í•¨ìˆ˜ ê²°ê³¼ë¥¼ íˆ¬ëª…í•˜ê²Œ ìºì‹±"""
    def decorator(func: Callable):
        def wrapper(*args, **kwargs):
            cache = get_cache()
            
            # ìºì‹œ í™•ì¸
            if file_path:
                cached_result = cache.get(file_path, operation_name, **cache_params)
                if cached_result is not None:
                    return cached_result
            
            # ìºì‹œê°€ ì—†ìœ¼ë©´ ì›ë³¸ í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ê²°ê³¼ ìºì‹±
            if file_path and result is not None:
                cache.set(file_path, operation_name, result, **cache_params)
            
            return result
        return wrapper
    return decorator

def cache_skeleton_extraction(file_path: str, extractor_func: Callable, *args, **kwargs):
    """ìŠ¤ì¼ˆë ˆí†¤ ì¶”ì¶œ ê²°ê³¼ ìºì‹± (ì˜¤ë¥˜ ë°©ì§€ ê°•í™”)"""
    try:
        cache = get_cache()
        
        # ğŸ” ìºì‹œ í™•ì¸ ë¡œê¹…
        logger.info(f"ğŸ“‚ ìºì‹œ í™•ì¸ ì¤‘: {Path(file_path).name}")
        cached_result = cache.get(file_path, "skeleton", **kwargs)
        if cached_result is not None:
            logger.info(f"âœ… ìºì‹œ íˆíŠ¸: {Path(file_path).name} - ìŠ¤ì¼ˆë ˆí†¤ ë¡œë“œë¨")
            return cached_result
        
        # ìºì‹œê°€ ì—†ìœ¼ë©´ ì›ë³¸ í•¨ìˆ˜ ì‹¤í–‰
        logger.info(f"ğŸ”„ ìŠ¤ì¼ˆë ˆí†¤ ì¶”ì¶œ ì¤‘: {Path(file_path).name}")
        result = extractor_func(*args, **kwargs)
        
        # ğŸ” ê²°ê³¼ ìºì‹± ë¡œê¹…
        if result is not None:
            cache.set(file_path, "skeleton", result, **kwargs)
            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥ë¨: {Path(file_path).name}")
        else:
            logger.warning(f"âš ï¸ ë¹ˆ ê²°ê³¼ë¡œ ìºì‹œ ì €ì¥ ì•ˆë¨: {Path(file_path).name}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì˜¤ë¥˜ - ì§ì ‘ ì‹¤í–‰: {e}")
        # ìºì‹œ ì˜¤ë¥˜ ì‹œ ì›ë³¸ í•¨ìˆ˜ ì§ì ‘ ì‹¤í–‰
        return extractor_func(*args, **kwargs)

def cache_ai_prediction(file_path: str, model_path: str, predictor_func: Callable, skeleton, *args, **kwargs):
    """AI ì˜ˆì¸¡ ê²°ê³¼ ìºì‹± (ì˜¤ë¥˜ ë°©ì§€ ê°•í™”)"""
    try:
        cache = get_cache()
        
        # ìŠ¤ì¼ˆë ˆí†¤ í•´ì‹œë¥¼ í¬í•¨í•œ ìºì‹œ í‚¤ ìƒì„±
        skeleton_hash = hashlib.md5(str(skeleton).encode()).hexdigest()[:16]
        model_hash = get_cache()._get_file_hash(model_path) if os.path.exists(model_path) else "no_model"
        
        cache_params = {
            "skeleton_hash": skeleton_hash,
            "model_hash": model_hash,
            **kwargs
        }
        
        # ğŸ” ìºì‹œ í™•ì¸ ë¡œê¹…
        logger.info(f"ğŸ¤– AI ì˜ˆì¸¡ ìºì‹œ í™•ì¸ ì¤‘: {Path(file_path).name}")
        cached_result = cache.get(file_path, "ai_prediction", **cache_params)
        if cached_result is not None:
            logger.info(f"âœ… AI ì˜ˆì¸¡ ìºì‹œ íˆíŠ¸: {Path(file_path).name}")
            return cached_result
        
        # ìºì‹œê°€ ì—†ìœ¼ë©´ ì›ë³¸ í•¨ìˆ˜ ì‹¤í–‰
        logger.info(f"ğŸ¤– AI ì˜ˆì¸¡ ì¤‘: {Path(file_path).name}")
        result = predictor_func(skeleton, *args, **kwargs)
        
        # ğŸ” ê²°ê³¼ ìºì‹± ë¡œê¹…
        if result is not None:
            cache.set(file_path, "ai_prediction", result, **cache_params)
            logger.info(f"ğŸ’¾ AI ì˜ˆì¸¡ ìºì‹œ ì €ì¥ë¨: {Path(file_path).name}")
        else:
            logger.warning(f"âš ï¸ AI ì˜ˆì¸¡ ë¹ˆ ê²°ê³¼ë¡œ ìºì‹œ ì €ì¥ ì•ˆë¨: {Path(file_path).name}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ AI ì˜ˆì¸¡ ìºì‹œ ì˜¤ë¥˜ - ì§ì ‘ ì‹¤í–‰: {e}")
        # ìºì‹œ ì˜¤ë¥˜ ì‹œ ì›ë³¸ í•¨ìˆ˜ ì§ì ‘ ì‹¤í–‰
        return predictor_func(skeleton, *args, **kwargs)

def cache_road_clipping(district_file: str, road_folder: str, clipper_func: Callable, *args, **kwargs):
    """ë„ë¡œë§ í´ë¦¬í•‘ ê²°ê³¼ ìºì‹±"""
    cache = get_cache()
    
    # ë„ë¡œ í´ë”ì˜ ìˆ˜ì • ì‹œê°„ë„ ê³ ë ¤
    try:
        road_folder_stat = os.stat(road_folder)
        road_folder_hash = hashlib.md5(f"{road_folder}_{road_folder_stat.st_mtime}".encode()).hexdigest()[:16]
    except:
        road_folder_hash = hashlib.md5(road_folder.encode()).hexdigest()[:16]
    
    cache_params = {
        "road_folder_hash": road_folder_hash,
        **kwargs
    }
    
    # ìºì‹œ í™•ì¸
    cached_result = cache.get(district_file, "clipping", **cache_params)
    if cached_result is not None:
        return cached_result
    
    # ìºì‹œê°€ ì—†ìœ¼ë©´ ì›ë³¸ í•¨ìˆ˜ ì‹¤í–‰
    logger.info(f"âœ‚ï¸ ë„ë¡œë§ í´ë¦¬í•‘ ì¤‘: {Path(district_file).name}")
    result = clipper_func(*args, **kwargs)
    
    # ê²°ê³¼ ìºì‹±
    cache.set(district_file, "clipping", result, **cache_params)
    
    return result 