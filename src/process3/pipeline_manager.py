"""
íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ ëª¨ë“ˆ - ì „ì²´ ìë™í™” íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
"""

import os
import tempfile
from typing import Dict, List, Tuple, Optional, Any, Callable
import logging
import numpy as np

from .data_processor import DataProcessor
from .filter_manager import FilterManager
from .ai_predictor import AIPredictor
from .session_manager import SessionManager

logger = logging.getLogger(__name__)


class PipelineManager:
    """íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 model_path: Optional[str] = None,
                 session_dir: str = "sessions",
                 filter_config: Optional[Dict[str, float]] = None):
        """
        Args:
            model_path: AI ëª¨ë¸ ê²½ë¡œ
            session_dir: ì„¸ì…˜ ì €ì¥ ë””ë ‰í† ë¦¬
            filter_config: í•„í„° ì„¤ì • {'dbscan_eps': 20.0, 'network_max_dist': 50.0, 'road_buffer': 2.0}
        """
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.data_processor = DataProcessor()
        
        # í•„í„° ì„¤ì •
        filter_config = filter_config or {}
        self.filter_manager = FilterManager(
            dbscan_eps=filter_config.get('dbscan_eps', 20.0),
            network_max_dist=filter_config.get('network_max_dist', 50.0),
            road_buffer=filter_config.get('road_buffer', 2.0)
        )
        
        self.ai_predictor = AIPredictor(model_path)
        self.session_manager = SessionManager(session_dir)
        
        # íŒŒì´í”„ë¼ì¸ ì„¤ì •
        self.pipeline_config = {
            'auto_filter': True,
            'ai_confidence_threshold': 0.7,
            'curve_detection_method': 'boundary_based',  # 'boundary_based' or 'heuristic'
            'enable_distance_calculation': True,
            'save_intermediate_results': False
        }
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        self.progress_callback: Optional[Callable[[int, str], None]] = None
        self.result_callback: Optional[Callable[[Dict[str, Any]], None]] = None
        
        logger.info("PipelineManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def set_progress_callback(self, callback: Callable[[int, str], None]):
        """ì§„í–‰ë¥  ì½œë°± ì„¤ì •"""
        self.progress_callback = callback
    
    def set_result_callback(self, callback: Callable[[Dict[str, Any]], None]):
        """ê²°ê³¼ ì½œë°± ì„¤ì •"""
        self.result_callback = callback
    
    def _emit_progress(self, progress: int, message: str):
        """ì§„í–‰ë¥  ì „ì†¡"""
        if self.progress_callback:
            self.progress_callback(progress, message)
        logger.info(f"ì§„í–‰ë¥  {progress}%: {message}")
    
    def _emit_result(self, result: Dict[str, Any]):
        """ê²°ê³¼ ì „ì†¡"""
        if self.result_callback:
            self.result_callback(result)
    
    def run_road_pipeline(self, file_path: str, 
                         enable_ai: bool = True,
                         save_session: bool = True,
                         target_crs: str = 'EPSG:5186') -> Dict[str, Any]:
        """
        ë„ë¡œë§ íŒŒì¼ ìë™í™” íŒŒì´í”„ë¼ì¸
        
        Args:
            file_path: ë„ë¡œë§ íŒŒì¼ ê²½ë¡œ
            enable_ai: AI ì˜ˆì¸¡ í™œì„±í™” ì—¬ë¶€
            save_session: ì„¸ì…˜ ì €ì¥ ì—¬ë¶€
            target_crs: ëŒ€ìƒ ì¢Œí‘œê³„ (ê¸°ë³¸ê°’: EPSG:5186)
        
        Returns:
            íŒŒì´í”„ë¼ì¸ ê²°ê³¼
        """
        try:
            self._emit_progress(0, "ë„ë¡œë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            
            # 1ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬
            self._emit_progress(10, "ğŸ” AIê°€ ë„ë¡œ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            data_result = self.data_processor.process_road_file(file_path, target_crs)
            
            if not data_result['success']:
                return {'success': False, 'error': data_result['error']}
            
            skeleton = data_result['skeleton']
            intersections = data_result['intersections']
            road_gdf = data_result['road_gdf']
            
            # 2ë‹¨ê³„: íœ´ë¦¬ìŠ¤í‹± ë¶„ì„
            self._emit_progress(20, "ğŸ¯ AIê°€ ë„ë¡œ ëì  ê²€ì¶œ ì¤‘...")
            endpoints = self.data_processor.detect_heuristic_endpoints(skeleton)
            
            # 3ë‹¨ê³„: ì»¤ë¸Œ ê²€ì¶œ
            self._emit_progress(30, "ğŸ”„ AIê°€ ì»¤ë¸Œì  ê²€ì¶œ ì¤‘...")
            if self.pipeline_config['curve_detection_method'] == 'boundary_based':
                curves = self.data_processor.detect_boundary_based_curves(
                    skeleton,
                    sample_distance=15.0,
                    curvature_threshold=0.20,
                    road_buffer=3.0,
                    cluster_radius=20.0
                )
                # êµì°¨ì  ê·¼ì²˜ ì»¤ë¸Œì  ì œê±°
                curves = self.data_processor.remove_curves_near_intersections(
                    curves, intersections, threshold=10.0
                )
            else:
                curves = []  # ë‹¤ë¥¸ ë°©ë²•ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¶”ê°€
            
            # 4ë‹¨ê³„: ì´ˆê¸° ì  êµ¬ì„±
            self._emit_progress(40, "ğŸ“ AI ë¶„ì„ ê²°ê³¼ ì •ë¦¬ ì¤‘...")
            initial_points = {
                'intersection': [(float(x), float(y)) for x, y in intersections],
                'curve': curves,
                'endpoint': endpoints
            }
            
            # 5ë‹¨ê³„: AI ì˜ˆì¸¡ (ì„ íƒì‚¬í•­)
            ai_result = None
            if enable_ai and self.ai_predictor.is_model_loaded():
                self._emit_progress(50, "ğŸ¤– AI ìŠ¤ë§ˆíŠ¸ ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...")
                ai_result = self.ai_predictor.predict_points(
                    skeleton, 
                    confidence_threshold=self.pipeline_config['ai_confidence_threshold']
                )
                
                if ai_result and ai_result['success']:
                    # AI ì‚­ì œ í¬ì¸íŠ¸ ì ìš©
                    delete_points = ai_result['ai_points'].get('delete', [])
                    if delete_points:
                        initial_points = self.ai_predictor.apply_deletions(
                            initial_points, delete_points
                        )
            
            # 6ë‹¨ê³„: í•„í„°ë§
            self._emit_progress(60, "ğŸ”§ AI ìµœì í™” í•„í„°ë§ ì¤‘...")
            if self.pipeline_config['auto_filter']:
                filtered_points = self.filter_manager.remove_duplicate_points(
                    initial_points, skeleton
                )
            else:
                filtered_points = initial_points
            
            # 7ë‹¨ê³„: ê±°ë¦¬ ê³„ì‚°
            distances_info = {}
            if self.pipeline_config['enable_distance_calculation']:
                self._emit_progress(70, "ğŸ“ AI ê±°ë¦¬ ê³„ì‚° ì¤‘...")
                distances_info = self._calculate_distances(filtered_points)
            
            # 8ë‹¨ê³„: ì„¸ì…˜ ì €ì¥
            session_path = None
            if save_session:
                self._emit_progress(80, "ğŸ’¾ AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
                session_path = self.session_manager.save_session(
                    file_path=file_path,
                    points=filtered_points,
                    skeleton=skeleton,
                    metadata={
                        'file_mode': 'road',
                        'pipeline_config': self.pipeline_config,
                        'ai_enabled': enable_ai,
                        'ai_result': ai_result is not None
                    }
                )
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'success': True,
                'skeleton': skeleton,
                'points': filtered_points,
                'road_gdf': road_gdf,
                'ai_result': ai_result,
                'distances': distances_info,
                'session_path': session_path,
                'stats': {
                    'total_skeleton_points': len(skeleton),
                    'detected_intersections': len(intersections),
                    'detected_curves': len(curves),
                    'detected_endpoints': len(endpoints),
                    'final_points': {
                        category: len(points) for category, points in filtered_points.items()
                    }
                }
            }
            
            self._emit_progress(100, "âœ… AI ë„ë¡œ ë¶„ì„ ì™„ë£Œ!")
            self._emit_result(result)
            
            return result
            
        except Exception as e:
            logger.error(f"ë„ë¡œë§ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_district_pipeline(self, file_path: str, 
                            target_crs: str = 'EPSG:5186',
                            enable_ai: bool = True,
                            save_session: bool = True) -> Dict[str, Any]:
        """
        ì§€êµ¬ê³„ íŒŒì¼ ìë™í™” íŒŒì´í”„ë¼ì¸
        
        Args:
            file_path: ì§€êµ¬ê³„ íŒŒì¼ ê²½ë¡œ
            target_crs: ëª©í‘œ ì¢Œí‘œê³„
            enable_ai: AI ì˜ˆì¸¡ í™œì„±í™” ì—¬ë¶€
            save_session: ì„¸ì…˜ ì €ì¥ ì—¬ë¶€
        
        Returns:
            íŒŒì´í”„ë¼ì¸ ê²°ê³¼
        """
        try:
            self._emit_progress(0, "ğŸŒ AI ì§€êµ¬ê³„ ë¶„ì„ ì‹œì‘")
            
            # 1ë‹¨ê³„: ì§€êµ¬ê³„ íŒŒì¼ ì²˜ë¦¬
            self._emit_progress(10, "ğŸ” AIê°€ ì§€êµ¬ê³„ ë„ë¡œë§ ë¶„ì„ ì¤‘...")
            district_result = self.data_processor.process_district_file(file_path, target_crs)
            
            if not district_result['success']:
                return {'success': False, 'error': district_result['error']}
            
            polygons = district_result['polygons']
            total_polygons = district_result['total_polygons']
            
            # 2ë‹¨ê³„: ê° í´ë¦¬ê³¤ ì²˜ë¦¬
            self._emit_progress(20, f"ğŸ—ºï¸ AI ì§€êµ¬ ë¶„ì„ ì‹œì‘ ({total_polygons}ê°œ êµ¬ì—­)")
            polygon_results = []
            
            for i, polygon_data in enumerate(polygons):
                self._emit_progress(
                    20 + (i * 60 // total_polygons), 
                    f"ğŸ¤– AI êµ¬ì—­ {i+1}/{total_polygons} ë¶„ì„ ì¤‘..."
                )
                
                # í´ë¦¬ê³¤ì— ë„ë¡œë§ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                if 'clipped_road' not in polygon_data or polygon_data['clipped_road'] is None:
                    continue
                
                # ì„ì‹œ íŒŒì¼ ìƒì„±í•˜ì—¬ ì²˜ë¦¬
                temp_path = self.data_processor.create_temporary_file(polygon_data['clipped_road'])
                
                try:
                    # ë„ë¡œë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                    polygon_result = self.run_road_pipeline(
                        temp_path, 
                        enable_ai=enable_ai, 
                        save_session=False,  # ê°œë³„ ì €ì¥ì€ ë‚˜ì¤‘ì—
                        target_crs=target_crs  # ì¢Œí‘œê³„ ì „ë‹¬
                    )
                    
                    if polygon_result['success']:
                        # í´ë¦¬ê³¤ ì •ë³´ ì¶”ê°€
                        # geometry_gdfê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ geometry ì‚¬ìš©
                        poly_geometry = polygon_data.get('geometry')
                        if 'geometry_gdf' in polygon_data and polygon_data['geometry_gdf'] is not None:
                            # GeoDataFrameì—ì„œ geometry ì¶”ì¶œ
                            poly_gdf = polygon_data['geometry_gdf']
                            if not poly_gdf.empty:
                                poly_geometry = poly_gdf.geometry.iloc[0]
                        
                        polygon_result['polygon_info'] = {
                            'index': i + 1,
                            'total': total_polygons,
                            'geometry': poly_geometry
                        }
                        
                        # í´ë¦¬ê³¤ ì„¸ì…˜ ì €ì¥
                        if save_session:
                            session_path = self.session_manager.save_polygon_session(
                                file_path=file_path,
                                points=polygon_result['points'],
                                skeleton=polygon_result['skeleton'],
                                polygon_index=i + 1,
                                total_polygons=total_polygons,
                                target_crs=target_crs
                            )
                            polygon_result['session_path'] = session_path
                        
                        polygon_results.append(polygon_result)
                    
                finally:
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
            
            # ê²°ê³¼ í†µí•©
            result = {
                'success': True,
                'file_mode': 'district',
                'target_crs': target_crs,
                'total_polygons': total_polygons,
                'processed_polygons': len(polygon_results),
                'polygon_results': polygon_results,
                'summary_stats': self._calculate_summary_stats(polygon_results)
            }
            
            self._emit_progress(100, "âœ… AI ì§€êµ¬ê³„ ë¶„ì„ ì™„ë£Œ!")
            self._emit_result(result)
            
            return result
            
        except Exception as e:
            logger.error(f"ì§€êµ¬ê³„ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_batch_pipeline(self, file_paths: List[str], 
                          file_mode: str = 'road',
                          enable_ai: bool = True,
                          save_sessions: bool = True,
                          target_crs: str = 'EPSG:5186') -> Dict[str, Any]:
        """
        ë°°ì¹˜ íŒŒì´í”„ë¼ì¸
        
        Args:
            file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            file_mode: íŒŒì¼ ëª¨ë“œ ('road' or 'district')
            enable_ai: AI ì˜ˆì¸¡ í™œì„±í™” ì—¬ë¶€
            save_sessions: ì„¸ì…˜ ì €ì¥ ì—¬ë¶€
        
        Returns:
            ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            self._emit_progress(0, f"ğŸš€ AI ë°°ì¹˜ ë¶„ì„ ì‹œì‘ ({len(file_paths)}ê°œ íŒŒì¼)")
            
            batch_results = []
            successful_count = 0
            failed_count = 0
            
            for i, file_path in enumerate(file_paths):
                self._emit_progress(
                    (i * 90 // len(file_paths)), 
                    f"ğŸ¤– AI íŒŒì¼ {i+1}/{len(file_paths)} ë¶„ì„ ì¤‘..."
                )
                
                try:
                    if file_mode == 'district':
                        result = self.run_district_pipeline(
                            file_path, 
                            target_crs=target_crs,  # ì¢Œí‘œê³„ ì „ë‹¬
                            enable_ai=enable_ai, 
                            save_session=save_sessions
                        )
                    else:
                        result = self.run_road_pipeline(
                            file_path, 
                            enable_ai=enable_ai, 
                            save_session=save_sessions,
                            target_crs=target_crs  # ì¢Œí‘œê³„ ì „ë‹¬
                        )
                    
                    if result['success']:
                        successful_count += 1
                    else:
                        failed_count += 1
                    
                    batch_results.append({
                        'file_path': file_path,
                        'result': result
                    })
                    
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                    failed_count += 1
                    batch_results.append({
                        'file_path': file_path,
                        'result': {'success': False, 'error': str(e)}
                    })
            
            # ë°°ì¹˜ ê²°ê³¼ í†µí•©
            batch_result = {
                'success': True,
                'total_files': len(file_paths),
                'successful_files': successful_count,
                'failed_files': failed_count,
                'file_mode': file_mode,
                'batch_results': batch_results,
                'summary_stats': self._calculate_batch_summary_stats(batch_results)
            }
            
            self._emit_progress(100, f"âœ… AI ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ! ({successful_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨)")
            self._emit_result(batch_result)
            
            return batch_result
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_distances(self, points: Dict[str, List[Tuple[float, float]]]) -> Dict[str, Any]:
        """ê±°ë¦¬ ê³„ì‚°"""
        try:
            import networkx as nx
            
            # ëª¨ë“  ì  ìˆ˜ì§‘
            all_points = []
            for category, point_list in points.items():
                all_points.extend(point_list)
            
            if len(all_points) < 2:
                return {'error': 'ì ì´ ë¶€ì¡±í•©ë‹ˆë‹¤'}
            
            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            G.add_nodes_from(range(len(all_points)))
            
            # ê°€ê¹Œìš´ ì ë“¤ ì—°ê²° (50m ì´ë‚´)
            total_distance = 0
            connections = []
            
            for i in range(len(all_points)):
                for j in range(i + 1, len(all_points)):
                    dist = np.hypot(all_points[i][0] - all_points[j][0], 
                                  all_points[i][1] - all_points[j][1])
                    if dist <= 50:  # 50m ì´ë‚´ë§Œ ì—°ê²°
                        G.add_edge(i, j, weight=dist)
                        connections.append((i, j, dist))
                        total_distance += dist
            
            # ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ë¶„ì„
            components = list(nx.connected_components(G))
            
            # ê±°ë¦¬ í†µê³„
            if connections:
                distances = [d for _, _, d in connections]
                min_dist = min(distances)
                max_dist = max(distances)
                avg_dist = sum(distances) / len(distances)
                
                return {
                    'total_points': len(all_points),
                    'total_connections': len(connections),
                    'total_distance': total_distance,
                    'min_distance': min_dist,
                    'max_distance': max_dist,
                    'avg_distance': avg_dist,
                    'network_components': len(components)
                }
            else:
                return {'error': 'ì—°ê²°ëœ ì ì´ ì—†ìŠµë‹ˆë‹¤'}
                
        except Exception as e:
            logger.error(f"ê±°ë¦¬ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def _calculate_summary_stats(self, polygon_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í´ë¦¬ê³¤ ê²°ê³¼ ìš”ì•½ í†µê³„"""
        if not polygon_results:
            return {}
        
        try:
            total_points = {'intersection': 0, 'curve': 0, 'endpoint': 0}
            total_skeleton_points = 0
            
            for result in polygon_results:
                if result['success']:
                    for category, points in result['points'].items():
                        total_points[category] += len(points)
                    total_skeleton_points += len(result['skeleton'])
            
            return {
                'total_polygons_processed': len(polygon_results),
                'total_skeleton_points': total_skeleton_points,
                'total_detected_points': total_points,
                'avg_points_per_polygon': {
                    category: count / len(polygon_results) 
                    for category, count in total_points.items()
                }
            }
            
        except Exception as e:
            logger.error(f"ìš”ì•½ í†µê³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def _calculate_batch_summary_stats(self, batch_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë°°ì¹˜ ê²°ê³¼ ìš”ì•½ í†µê³„"""
        if not batch_results:
            return {}
        
        try:
            successful_results = [r for r in batch_results if r['result']['success']]
            
            if not successful_results:
                return {'error': 'ì„±ê³µí•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            total_points = {'intersection': 0, 'curve': 0, 'endpoint': 0}
            total_skeleton_points = 0
            
            for batch_item in successful_results:
                result = batch_item['result']
                
                if 'points' in result:
                    for category, points in result['points'].items():
                        total_points[category] += len(points)
                    total_skeleton_points += len(result.get('skeleton', []))
                elif 'polygon_results' in result:
                    # ì§€êµ¬ê³„ ëª¨ë“œ
                    for poly_result in result['polygon_results']:
                        if poly_result['success']:
                            for category, points in poly_result['points'].items():
                                total_points[category] += len(points)
                            total_skeleton_points += len(poly_result['skeleton'])
            
            return {
                'total_successful_files': len(successful_results),
                'total_skeleton_points': total_skeleton_points,
                'total_detected_points': total_points,
                'avg_points_per_file': {
                    category: count / len(successful_results) 
                    for category, count in total_points.items()
                }
            }
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìš”ì•½ í†µê³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def update_config(self, config: Dict[str, Any]):
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.pipeline_config.update(config)
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸: {config}")
    
    def get_config(self) -> Dict[str, Any]:
        """í˜„ì¬ íŒŒì´í”„ë¼ì¸ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_config.copy()
    
    def get_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´"""
        return {
            'ai_model_loaded': self.ai_predictor.is_model_loaded(),
            'ai_model_path': self.ai_predictor.model_path,
            'session_dir': str(self.session_manager.session_dir),
            'pipeline_config': self.pipeline_config,
            'session_stats': self.session_manager.get_session_stats()
        } 